pub mod attribution_source_id;
pub mod cache_service_client;
pub mod cache_service_server;
pub mod cached_content;
pub mod candidate;
pub mod chunk;
pub mod chunk_data;
pub mod code_execution_result;
pub mod condition;
pub mod content_filter;
pub mod custom_metadata;
pub mod dataset;
pub mod dynamic_retrieval_config;
pub mod executable_code;
pub mod file;
pub mod file_service_client;
pub mod file_service_server;
pub mod function_calling_config;
pub mod generate_answer_request;
pub mod generate_answer_response;
pub mod generate_content_response;
pub mod generation_config;
pub mod generative_service_client;
pub mod generative_service_server;
pub mod grounding_chunk;
pub mod hyperparameters;
pub mod logprobs_result;
pub mod longrunning;
pub mod model_service_client;
pub mod model_service_server;
pub mod part;
pub mod rpc;
pub mod safety_rating;
pub mod safety_setting;
pub mod tool;
pub mod tuned_model;
pub mod tuning_example;
pub mod voice_config;

// use prost_types;
// This file is @generated by prost-build.
/// A collection of source attributions for a piece of content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CitationMetadata {
    /// Citations to sources for a specific response.
    #[prost(message, repeated, tag = "1")]
    pub citation_sources: ::prost::alloc::vec::Vec<CitationSource>,
}
/// A citation to a source for a portion of a specific response.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CitationSource {
    /// Optional. Start of segment of the response that is attributed to this
    /// source.
    ///
    /// Index indicates the start of the segment, measured in bytes.
    #[prost(int32, optional, tag = "1")]
    pub start_index: ::core::option::Option<i32>,
    /// Optional. End of the attributed segment, exclusive.
    #[prost(int32, optional, tag = "2")]
    pub end_index: ::core::option::Option<i32>,
    /// Optional. URI that is attributed as a source for a portion of the text.
    #[prost(string, optional, tag = "3")]
    pub uri: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. License for the GitHub project that is attributed as a source for
    /// segment.
    ///
    /// License info is required for code citations.
    #[prost(string, optional, tag = "4")]
    pub license: ::core::option::Option<::prost::alloc::string::String>,
}
/// The base structured datatype containing multi-part content of a message.
///
/// A `Content` includes a `role` field designating the producer of the `Content`
/// and a `parts` field containing multi-part data that contains the content of
/// the message turn.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Content {
    /// Ordered `Parts` that constitute a single message. Parts may have different
    /// MIME types.
    #[prost(message, repeated, tag = "1")]
    pub parts: ::prost::alloc::vec::Vec<Part>,
    /// Optional. The producer of the content. Must be either 'user' or 'model'.
    ///
    /// Useful to set for multi-turn conversations, otherwise can be left blank
    /// or unset.
    #[prost(string, tag = "2")]
    pub role: ::prost::alloc::string::String,
}
/// A datatype containing media that is part of a multi-part `Content` message.
///
/// A `Part` consists of data which has an associated datatype. A `Part` can only
/// contain one of the accepted types in `Part.data`.
///
/// A `Part` must have a fixed IANA MIME type identifying the type and subtype
/// of the media if the `inline_data` field is filled with raw bytes.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Part {
    #[prost(oneof = "part::Data", tags = "2, 3, 4, 5, 6, 9, 10")]
    pub data: ::core::option::Option<part::Data>,
}

/// Raw media bytes.
///
/// Text should not be sent as raw bytes, use the 'text' field.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Blob {
    /// The IANA standard MIME type of the source data.
    /// Examples:
    ///    - image/png
    ///    - image/jpeg
    /// If an unsupported MIME type is provided, an error will be returned. For a
    /// complete list of supported types, see [Supported file
    /// formats](<https://ai.google.dev/gemini-api/docs/prompting_with_media#supported_file_formats>).
    #[prost(string, tag = "1")]
    pub mime_type: ::prost::alloc::string::String,
    /// Raw bytes for media formats.
    #[prost(bytes = "vec", tag = "2")]
    pub data: ::prost::alloc::vec::Vec<u8>,
}
/// URI based data.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FileData {
    /// Optional. The IANA standard MIME type of the source data.
    #[prost(string, tag = "1")]
    pub mime_type: ::prost::alloc::string::String,
    /// Required. URI.
    #[prost(string, tag = "2")]
    pub file_uri: ::prost::alloc::string::String,
}
/// Code generated by the model that is meant to be executed, and the result
/// returned to the model.
///
/// Only generated when using the `CodeExecution` tool, in which the code will
/// be automatically executed, and a corresponding `CodeExecutionResult` will
/// also be generated.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExecutableCode {
    /// Required. Programming language of the `code`.
    #[prost(enumeration = "executable_code::Language", tag = "1")]
    pub language: i32,
    /// Required. The code to be executed.
    #[prost(string, tag = "2")]
    pub code: ::prost::alloc::string::String,
}

/// Result of executing the `ExecutableCode`.
///
/// Only generated when using the `CodeExecution`, and always follows a `part`
/// containing the `ExecutableCode`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CodeExecutionResult {
    /// Required. Outcome of the code execution.
    #[prost(enumeration = "code_execution_result::Outcome", tag = "1")]
    pub outcome: i32,
    /// Optional. Contains stdout when code execution is successful, stderr or
    /// other description otherwise.
    #[prost(string, tag = "2")]
    pub output: ::prost::alloc::string::String,
}

/// Tool details that the model may use to generate response.
///
/// A `Tool` is a piece of code that enables the system to interact with
/// external systems to perform an action, or set of actions, outside of
/// knowledge and scope of the model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Tool {
    /// Optional. A list of `FunctionDeclarations` available to the model that can
    /// be used for function calling.
    ///
    /// The model or system does not execute the function. Instead the defined
    /// function may be returned as a
    /// [FunctionCall][google.ai.generativelanguage.v1beta.Part.function_call] with
    /// arguments to the client side for execution. The model may decide to call a
    /// subset of these functions by populating
    /// [FunctionCall][google.ai.generativelanguage.v1beta.Part.function_call] in
    /// the response. The next conversation turn may contain a
    /// [FunctionResponse][google.ai.generativelanguage.v1beta.Part.function_response]
    /// with the [Content.role][google.ai.generativelanguage.v1beta.Content.role]
    /// "function" generation context for the next model turn.
    #[prost(message, repeated, tag = "1")]
    pub function_declarations: ::prost::alloc::vec::Vec<FunctionDeclaration>,
    /// Optional. Retrieval tool that is powered by Google search.
    #[prost(message, optional, tag = "2")]
    pub google_search_retrieval: ::core::option::Option<GoogleSearchRetrieval>,
    /// Optional. Enables the model to execute code as part of generation.
    #[prost(message, optional, tag = "3")]
    pub code_execution: ::core::option::Option<CodeExecution>,
    /// Optional. GoogleSearch tool type.
    /// Tool to support Google Search in Model. Powered by Google.
    #[prost(message, optional, tag = "4")]
    pub google_search: ::core::option::Option<tool::GoogleSearch>,
}

/// Tool to retrieve public web data for grounding, powered by Google.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct GoogleSearchRetrieval {
    /// Specifies the dynamic retrieval configuration for the given source.
    #[prost(message, optional, tag = "1")]
    pub dynamic_retrieval_config: ::core::option::Option<DynamicRetrievalConfig>,
}
/// Describes the options to customize dynamic retrieval.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct DynamicRetrievalConfig {
    /// The mode of the predictor to be used in dynamic retrieval.
    #[prost(enumeration = "dynamic_retrieval_config::Mode", tag = "1")]
    pub mode: i32,
    /// The threshold to be used in dynamic retrieval.
    /// If not set, a system default value is used.
    #[prost(float, optional, tag = "2")]
    pub dynamic_threshold: ::core::option::Option<f32>,
}

/// Tool that executes code generated by the model, and automatically returns
/// the result to the model.
///
/// See also `ExecutableCode` and `CodeExecutionResult` which are only generated
/// when using this tool.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct CodeExecution {}
/// The Tool configuration containing parameters for specifying `Tool` use
/// in the request.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ToolConfig {
    /// Optional. Function calling config.
    #[prost(message, optional, tag = "1")]
    pub function_calling_config: ::core::option::Option<FunctionCallingConfig>,
}
/// Configuration for specifying function calling behavior.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FunctionCallingConfig {
    /// Optional. Specifies the mode in which function calling should execute. If
    /// unspecified, the default value will be set to AUTO.
    #[prost(enumeration = "function_calling_config::Mode", tag = "1")]
    pub mode: i32,
    /// Optional. A set of function names that, when provided, limits the functions
    /// the model will call.
    ///
    /// This should only be set when the Mode is ANY. Function names
    /// should match \[FunctionDeclaration.name\]. With mode set to ANY, model will
    /// predict a function call from the set of function names provided.
    #[prost(string, repeated, tag = "2")]
    pub allowed_function_names: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}

/// Structured representation of a function declaration as defined by the
/// [OpenAPI 3.03 specification](<https://spec.openapis.org/oas/v3.0.3>). Included
/// in this declaration are the function name and parameters. This
/// FunctionDeclaration is a representation of a block of code that can be used
/// as a `Tool` by the model and executed by the client.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FunctionDeclaration {
    /// Required. The name of the function.
    /// Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum
    /// length of 63.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. A brief description of the function.
    #[prost(string, tag = "2")]
    pub description: ::prost::alloc::string::String,
    /// Optional. Describes the parameters to this function. Reflects the Open
    /// API 3.03 Parameter Object string Key: the name of the parameter. Parameter
    /// names are case sensitive. Schema Value: the Schema defining the type used
    /// for the parameter.
    #[prost(message, optional, tag = "3")]
    pub parameters: ::core::option::Option<Schema>,
    /// Optional. Describes the output from this function in JSON Schema format.
    /// Reflects the Open API 3.03 Response Object. The Schema defines the type
    /// used for the response value of the function.
    #[prost(message, optional, tag = "4")]
    pub response: ::core::option::Option<Schema>,
}
/// A predicted `FunctionCall` returned from the model that contains
/// a string representing the `FunctionDeclaration.name` with the
/// arguments and their values.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FunctionCall {
    /// Optional. The unique id of the function call. If populated, the client to
    /// execute the `function_call` and return the response with the matching `id`.
    #[prost(string, tag = "3")]
    pub id: ::prost::alloc::string::String,
    /// Required. The name of the function to call.
    /// Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum
    /// length of 63.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. The function parameters and values in JSON object format.
    #[prost(message, optional, tag = "2")]
    pub args: ::core::option::Option<::prost_types::Struct>,
}
/// The result output from a `FunctionCall` that contains a string
/// representing the `FunctionDeclaration.name` and a structured JSON
/// object containing any output from the function is used as context to
/// the model. This should contain the result of a`FunctionCall` made
/// based on model prediction.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FunctionResponse {
    /// Optional. The id of the function call this response is for. Populated by
    /// the client to match the corresponding function call `id`.
    #[prost(string, tag = "3")]
    pub id: ::prost::alloc::string::String,
    /// Required. The name of the function to call.
    /// Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum
    /// length of 63.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The function response in JSON object format.
    #[prost(message, optional, tag = "2")]
    pub response: ::core::option::Option<::prost_types::Struct>,
}
/// The `Schema` object allows the definition of input and output data types.
/// These types can be objects, but also primitives and arrays.
/// Represents a select subset of an [OpenAPI 3.0 schema
/// object](<https://spec.openapis.org/oas/v3.0.3#schema>).
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Schema {
    /// Required. Data type.
    #[prost(enumeration = "Type", tag = "1")]
    pub r#type: i32,
    /// Optional. The format of the data. This is used only for primitive
    /// datatypes. Supported formats:
    ///   for NUMBER type: float, double
    ///   for INTEGER type: int32, int64
    ///   for STRING type: enum
    #[prost(string, tag = "2")]
    pub format: ::prost::alloc::string::String,
    /// Optional. A brief description of the parameter. This could contain examples
    /// of use. Parameter description may be formatted as Markdown.
    #[prost(string, tag = "3")]
    pub description: ::prost::alloc::string::String,
    /// Optional. Indicates if the value may be null.
    #[prost(bool, tag = "4")]
    pub nullable: bool,
    /// Optional. Possible values of the element of Type.STRING with enum format.
    /// For example we can define an Enum Direction as :
    /// {type:STRING, format:enum, enum:\["EAST", NORTH", "SOUTH", "WEST"\]}
    #[prost(string, repeated, tag = "5")]
    pub r#enum: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional. Schema of the elements of Type.ARRAY.
    #[prost(message, optional, boxed, tag = "6")]
    pub items: ::core::option::Option<::prost::alloc::boxed::Box<Schema>>,
    /// Optional. Maximum number of the elements for Type.ARRAY.
    #[prost(int64, tag = "21")]
    pub max_items: i64,
    /// Optional. Minimum number of the elements for Type.ARRAY.
    #[prost(int64, tag = "22")]
    pub min_items: i64,
    /// Optional. Properties of Type.OBJECT.
    #[prost(map = "string, message", tag = "7")]
    pub properties: ::std::collections::HashMap<::prost::alloc::string::String, Schema>,
    /// Optional. Required properties of Type.OBJECT.
    #[prost(string, repeated, tag = "8")]
    pub required: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Passage included inline with a grounding configuration.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundingPassage {
    /// Identifier for the passage for attributing this passage in grounded
    /// answers.
    #[prost(string, tag = "1")]
    pub id: ::prost::alloc::string::String,
    /// Content of the passage.
    #[prost(message, optional, tag = "2")]
    pub content: ::core::option::Option<Content>,
}
/// A repeated list of passages.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundingPassages {
    /// List of passages.
    #[prost(message, repeated, tag = "1")]
    pub passages: ::prost::alloc::vec::Vec<GroundingPassage>,
}
/// Type contains the list of OpenAPI data types as defined by
/// <https://spec.openapis.org/oas/v3.0.3#data-types>
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum Type {
    /// Not specified, should not be used.
    Unspecified = 0,
    /// String type.
    String = 1,
    /// Number type.
    Number = 2,
    /// Integer type.
    Integer = 3,
    /// Boolean type.
    Boolean = 4,
    /// Array type.
    Array = 5,
    /// Object type.
    Object = 6,
}
impl Type {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "TYPE_UNSPECIFIED",
            Self::String => "STRING",
            Self::Number => "NUMBER",
            Self::Integer => "INTEGER",
            Self::Boolean => "BOOLEAN",
            Self::Array => "ARRAY",
            Self::Object => "OBJECT",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "TYPE_UNSPECIFIED" => Some(Self::Unspecified),
            "STRING" => Some(Self::String),
            "NUMBER" => Some(Self::Number),
            "INTEGER" => Some(Self::Integer),
            "BOOLEAN" => Some(Self::Boolean),
            "ARRAY" => Some(Self::Array),
            "OBJECT" => Some(Self::Object),
            _ => None,
        }
    }
}
/// A `Corpus` is a collection of `Document`s.
/// A project can create up to 5 corpora.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Corpus {
    /// Immutable. Identifier. The `Corpus` resource name. The ID (name excluding
    /// the "corpora/" prefix) can contain up to 40 characters that are lowercase
    /// alphanumeric or dashes
    /// (-). The ID cannot start or end with a dash. If the name is empty on
    /// create, a unique name will be derived from `display_name` along with a 12
    /// character random suffix.
    /// Example: `corpora/my-awesome-corpora-123a456b789c`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. The human-readable display name for the `Corpus`. The display
    /// name must be no more than 512 characters in length, including spaces.
    /// Example: "Docs on Semantic Retriever"
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Output only. The Timestamp of when the `Corpus` was created.
    #[prost(message, optional, tag = "3")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The Timestamp of when the `Corpus` was last updated.
    #[prost(message, optional, tag = "4")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
}
/// A `Document` is a collection of `Chunk`s.
/// A `Corpus` can have a maximum of 10,000 `Document`s.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Document {
    /// Immutable. Identifier. The `Document` resource name. The ID (name excluding
    /// the "corpora/*/documents/" prefix) can contain up to 40 characters that are
    /// lowercase alphanumeric or dashes (-). The ID cannot start or end with a
    /// dash. If the name is empty on create, a unique name will be derived from
    /// `display_name` along with a 12 character random suffix.
    /// Example: `corpora/{corpus_id}/documents/my-awesome-doc-123a456b789c`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. The human-readable display name for the `Document`. The display
    /// name must be no more than 512 characters in length, including spaces.
    /// Example: "Semantic Retriever Documentation"
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Optional. User provided custom metadata stored as key-value pairs used for
    /// querying. A `Document` can have a maximum of 20 `CustomMetadata`.
    #[prost(message, repeated, tag = "3")]
    pub custom_metadata: ::prost::alloc::vec::Vec<CustomMetadata>,
    /// Output only. The Timestamp of when the `Document` was last updated.
    #[prost(message, optional, tag = "4")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The Timestamp of when the `Document` was created.
    #[prost(message, optional, tag = "5")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
}
/// User provided string values assigned to a single metadata key.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StringList {
    /// The string values of the metadata to store.
    #[prost(string, repeated, tag = "1")]
    pub values: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// User provided metadata stored as key-value pairs.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CustomMetadata {
    /// Required. The key of the metadata to store.
    #[prost(string, tag = "1")]
    pub key: ::prost::alloc::string::String,
    #[prost(oneof = "custom_metadata::Value", tags = "2, 6, 7")]
    pub value: ::core::option::Option<custom_metadata::Value>,
}

/// User provided filter to limit retrieval based on `Chunk` or `Document` level
/// metadata values.
/// Example (genre = drama OR genre = action):
///    key = "document.custom_metadata.genre"
///    conditions = [{string_value = "drama", operation = EQUAL},
///                  {string_value = "action", operation = EQUAL}]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MetadataFilter {
    /// Required. The key of the metadata to filter on.
    #[prost(string, tag = "1")]
    pub key: ::prost::alloc::string::String,
    /// Required. The `Condition`s for the given key that will trigger this filter.
    /// Multiple `Condition`s are joined by logical ORs.
    #[prost(message, repeated, tag = "2")]
    pub conditions: ::prost::alloc::vec::Vec<Condition>,
}
/// Filter condition applicable to a single key.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Condition {
    /// Required. Operator applied to the given key-value pair to trigger the
    /// condition.
    #[prost(enumeration = "condition::Operator", tag = "5")]
    pub operation: i32,
    /// The value type must be consistent with the value type defined in the field
    /// for the corresponding key. If the value types are not consistent, the
    /// result will be an empty set. When the `CustomMetadata` has a `StringList`
    /// value type, the filtering condition should use `string_value` paired with
    /// an INCLUDES/EXCLUDES operation, otherwise the result will also be an empty
    /// set.
    #[prost(oneof = "condition::Value", tags = "1, 6")]
    pub value: ::core::option::Option<condition::Value>,
}

/// A `Chunk` is a subpart of a `Document` that is treated as an independent unit
/// for the purposes of vector representation and storage.
/// A `Corpus` can have a maximum of 1 million `Chunk`s.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Chunk {
    /// Immutable. Identifier. The `Chunk` resource name. The ID (name excluding
    /// the "corpora/*/documents/*/chunks/" prefix) can contain up to 40 characters
    /// that are lowercase alphanumeric or dashes (-). The ID cannot start or end
    /// with a dash. If the name is empty on create, a random 12-character unique
    /// ID will be generated.
    /// Example: `corpora/{corpus_id}/documents/{document_id}/chunks/123a456b789c`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The content for the `Chunk`, such as the text string.
    /// The maximum number of tokens per chunk is 2043.
    #[prost(message, optional, tag = "2")]
    pub data: ::core::option::Option<ChunkData>,
    /// Optional. User provided custom metadata stored as key-value pairs.
    /// The maximum number of `CustomMetadata` per chunk is 20.
    #[prost(message, repeated, tag = "3")]
    pub custom_metadata: ::prost::alloc::vec::Vec<CustomMetadata>,
    /// Output only. The Timestamp of when the `Chunk` was created.
    #[prost(message, optional, tag = "4")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The Timestamp of when the `Chunk` was last updated.
    #[prost(message, optional, tag = "5")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Current state of the `Chunk`.
    #[prost(enumeration = "chunk::State", tag = "6")]
    pub state: i32,
}

/// Extracted data that represents the `Chunk` content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ChunkData {
    #[prost(oneof = "chunk_data::Data", tags = "1")]
    pub data: ::core::option::Option<chunk_data::Data>,
}

/// Content filtering metadata associated with processing a single request.
///
/// ContentFilter contains a reason and an optional supporting string. The reason
/// may be unspecified.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ContentFilter {
    /// The reason content was blocked during request processing.
    #[prost(enumeration = "content_filter::BlockedReason", tag = "1")]
    pub reason: i32,
    /// A string that describes the filtering behavior in more detail.
    #[prost(string, optional, tag = "2")]
    pub message: ::core::option::Option<::prost::alloc::string::String>,
}

/// Safety feedback for an entire request.
///
/// This field is populated if content in the input and/or response is blocked
/// due to safety settings. SafetyFeedback may not exist for every HarmCategory.
/// Each SafetyFeedback will return the safety settings used by the request as
/// well as the lowest HarmProbability that should be allowed in order to return
/// a result.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SafetyFeedback {
    /// Safety rating evaluated from content.
    #[prost(message, optional, tag = "1")]
    pub rating: ::core::option::Option<SafetyRating>,
    /// Safety settings applied to the request.
    #[prost(message, optional, tag = "2")]
    pub setting: ::core::option::Option<SafetySetting>,
}
/// Safety rating for a piece of content.
///
/// The safety rating contains the category of harm and the
/// harm probability level in that category for a piece of content.
/// Content is classified for safety across a number of
/// harm categories and the probability of the harm classification is included
/// here.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SafetyRating {
    /// Required. The category for this rating.
    #[prost(enumeration = "HarmCategory", tag = "3")]
    pub category: i32,
    /// Required. The probability of harm for this content.
    #[prost(enumeration = "safety_rating::HarmProbability", tag = "4")]
    pub probability: i32,
    /// Was this content blocked because of this rating?
    #[prost(bool, tag = "5")]
    pub blocked: bool,
}

/// Safety setting, affecting the safety-blocking behavior.
///
/// Passing a safety setting for a category changes the allowed probability that
/// content is blocked.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SafetySetting {
    /// Required. The category for this setting.
    #[prost(enumeration = "HarmCategory", tag = "3")]
    pub category: i32,
    /// Required. Controls the probability threshold at which harm is blocked.
    #[prost(enumeration = "safety_setting::HarmBlockThreshold", tag = "4")]
    pub threshold: i32,
}

/// The category of a rating.
///
/// These categories cover various kinds of harms that developers
/// may wish to adjust.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum HarmCategory {
    /// Category is unspecified.
    Unspecified = 0,
    /// **PaLM** - Negative or harmful comments targeting identity and/or protected
    /// attribute.
    Derogatory = 1,
    /// **PaLM** - Content that is rude, disrespectful, or profane.
    Toxicity = 2,
    /// **PaLM** - Describes scenarios depicting violence against an individual or
    /// group, or general descriptions of gore.
    Violence = 3,
    /// **PaLM** - Contains references to sexual acts or other lewd content.
    Sexual = 4,
    /// **PaLM** - Promotes unchecked medical advice.
    Medical = 5,
    /// **PaLM** - Dangerous content that promotes, facilitates, or encourages
    /// harmful acts.
    Dangerous = 6,
    /// **Gemini** - Harassment content.
    Harassment = 7,
    /// **Gemini** - Hate speech and content.
    HateSpeech = 8,
    /// **Gemini** - Sexually explicit content.
    SexuallyExplicit = 9,
    /// **Gemini** - Dangerous content.
    DangerousContent = 10,
    /// **Gemini** - Content that may be used to harm civic integrity.
    CivicIntegrity = 11,
}
impl HarmCategory {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "HARM_CATEGORY_UNSPECIFIED",
            Self::Derogatory => "HARM_CATEGORY_DEROGATORY",
            Self::Toxicity => "HARM_CATEGORY_TOXICITY",
            Self::Violence => "HARM_CATEGORY_VIOLENCE",
            Self::Sexual => "HARM_CATEGORY_SEXUAL",
            Self::Medical => "HARM_CATEGORY_MEDICAL",
            Self::Dangerous => "HARM_CATEGORY_DANGEROUS",
            Self::Harassment => "HARM_CATEGORY_HARASSMENT",
            Self::HateSpeech => "HARM_CATEGORY_HATE_SPEECH",
            Self::SexuallyExplicit => "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            Self::DangerousContent => "HARM_CATEGORY_DANGEROUS_CONTENT",
            Self::CivicIntegrity => "HARM_CATEGORY_CIVIC_INTEGRITY",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "HARM_CATEGORY_UNSPECIFIED" => Some(Self::Unspecified),
            "HARM_CATEGORY_DEROGATORY" => Some(Self::Derogatory),
            "HARM_CATEGORY_TOXICITY" => Some(Self::Toxicity),
            "HARM_CATEGORY_VIOLENCE" => Some(Self::Violence),
            "HARM_CATEGORY_SEXUAL" => Some(Self::Sexual),
            "HARM_CATEGORY_MEDICAL" => Some(Self::Medical),
            "HARM_CATEGORY_DANGEROUS" => Some(Self::Dangerous),
            "HARM_CATEGORY_HARASSMENT" => Some(Self::Harassment),
            "HARM_CATEGORY_HATE_SPEECH" => Some(Self::HateSpeech),
            "HARM_CATEGORY_SEXUALLY_EXPLICIT" => Some(Self::SexuallyExplicit),
            "HARM_CATEGORY_DANGEROUS_CONTENT" => Some(Self::DangerousContent),
            "HARM_CATEGORY_CIVIC_INTEGRITY" => Some(Self::CivicIntegrity),
            _ => None,
        }
    }
}
/// Request to generate a completion from the model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateContentRequest {
    /// Required. The name of the `Model` to use for generating the completion.
    ///
    /// Format: `models/{model}`.
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Optional. Developer set [system
    /// instruction(s)](<https://ai.google.dev/gemini-api/docs/system-instructions>).
    /// Currently, text only.
    #[prost(message, optional, tag = "8")]
    pub system_instruction: ::core::option::Option<Content>,
    /// Required. The content of the current conversation with the model.
    ///
    /// For single-turn queries, this is a single instance. For multi-turn queries
    /// like [chat](<https://ai.google.dev/gemini-api/docs/text-generation#chat>),
    /// this is a repeated field that contains the conversation history and the
    /// latest request.
    #[prost(message, repeated, tag = "2")]
    pub contents: ::prost::alloc::vec::Vec<Content>,
    /// Optional. A list of `Tools` the `Model` may use to generate the next
    /// response.
    ///
    /// A `Tool` is a piece of code that enables the system to interact with
    /// external systems to perform an action, or set of actions, outside of
    /// knowledge and scope of the `Model`. Supported `Tool`s are `Function` and
    /// `code_execution`. Refer to the [Function
    /// calling](<https://ai.google.dev/gemini-api/docs/function-calling>) and the
    /// [Code execution](<https://ai.google.dev/gemini-api/docs/code-execution>)
    /// guides to learn more.
    #[prost(message, repeated, tag = "5")]
    pub tools: ::prost::alloc::vec::Vec<Tool>,
    /// Optional. Tool configuration for any `Tool` specified in the request. Refer
    /// to the [Function calling
    /// guide](<https://ai.google.dev/gemini-api/docs/function-calling#function_calling_mode>)
    /// for a usage example.
    #[prost(message, optional, tag = "7")]
    pub tool_config: ::core::option::Option<ToolConfig>,
    /// Optional. A list of unique `SafetySetting` instances for blocking unsafe
    /// content.
    ///
    /// This will be enforced on the `GenerateContentRequest.contents` and
    /// `GenerateContentResponse.candidates`. There should not be more than one
    /// setting for each `SafetyCategory` type. The API will block any contents and
    /// responses that fail to meet the thresholds set by these settings. This list
    /// overrides the default settings for each `SafetyCategory` specified in the
    /// safety_settings. If there is no `SafetySetting` for a given
    /// `SafetyCategory` provided in the list, the API will use the default safety
    /// setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
    /// HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
    /// HARM_CATEGORY_HARASSMENT, HARM_CATEGORY_CIVIC_INTEGRITY are supported.
    /// Refer to the [guide](<https://ai.google.dev/gemini-api/docs/safety-settings>)
    /// for detailed information on available safety settings. Also refer to the
    /// [Safety guidance](<https://ai.google.dev/gemini-api/docs/safety-guidance>) to
    /// learn how to incorporate safety considerations in your AI applications.
    #[prost(message, repeated, tag = "3")]
    pub safety_settings: ::prost::alloc::vec::Vec<SafetySetting>,
    /// Optional. Configuration options for model generation and outputs.
    #[prost(message, optional, tag = "4")]
    pub generation_config: ::core::option::Option<GenerationConfig>,
    /// Optional. The name of the content
    /// [cached](<https://ai.google.dev/gemini-api/docs/caching>) to use as context
    /// to serve the prediction. Format: `cachedContents/{cachedContent}`
    #[prost(string, optional, tag = "9")]
    pub cached_content: ::core::option::Option<::prost::alloc::string::String>,
}
/// The configuration for the prebuilt speaker to use.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PrebuiltVoiceConfig {
    /// The name of the preset voice to use.
    #[prost(string, optional, tag = "1")]
    pub voice_name: ::core::option::Option<::prost::alloc::string::String>,
}
/// The configuration for the voice to use.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct VoiceConfig {
    /// The configuration for the speaker to use.
    #[prost(oneof = "voice_config::VoiceConfig", tags = "1")]
    pub voice_config: ::core::option::Option<voice_config::VoiceConfig>,
}

/// The speech generation config.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SpeechConfig {
    /// The configuration for the speaker to use.
    #[prost(message, optional, tag = "1")]
    pub voice_config: ::core::option::Option<VoiceConfig>,
}
/// Configuration options for model generation and outputs. Not all parameters
/// are configurable for every model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerationConfig {
    /// Optional. Number of generated responses to return.
    ///
    /// Currently, this value can only be set to 1. If unset, this will default
    /// to 1.
    #[prost(int32, optional, tag = "1")]
    pub candidate_count: ::core::option::Option<i32>,
    /// Optional. The set of character sequences (up to 5) that will stop output
    /// generation. If specified, the API will stop at the first appearance of a
    /// `stop_sequence`. The stop sequence will not be included as part of the
    /// response.
    #[prost(string, repeated, tag = "2")]
    pub stop_sequences: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional. The maximum number of tokens to include in a response candidate.
    ///
    /// Note: The default value varies by model, see the `Model.output_token_limit`
    /// attribute of the `Model` returned from the `getModel` function.
    #[prost(int32, optional, tag = "4")]
    pub max_output_tokens: ::core::option::Option<i32>,
    /// Optional. Controls the randomness of the output.
    ///
    /// Note: The default value varies by model, see the `Model.temperature`
    /// attribute of the `Model` returned from the `getModel` function.
    ///
    /// Values can range from \[0.0, 2.0\].
    #[prost(float, optional, tag = "5")]
    pub temperature: ::core::option::Option<f32>,
    /// Optional. The maximum cumulative probability of tokens to consider when
    /// sampling.
    ///
    /// The model uses combined Top-k and Top-p (nucleus) sampling.
    ///
    /// Tokens are sorted based on their assigned probabilities so that only the
    /// most likely tokens are considered. Top-k sampling directly limits the
    /// maximum number of tokens to consider, while Nucleus sampling limits the
    /// number of tokens based on the cumulative probability.
    ///
    /// Note: The default value varies by `Model` and is specified by
    /// the`Model.top_p` attribute returned from the `getModel` function. An empty
    /// `top_k` attribute indicates that the model doesn't apply top-k sampling
    /// and doesn't allow setting `top_k` on requests.
    #[prost(float, optional, tag = "6")]
    pub top_p: ::core::option::Option<f32>,
    /// Optional. The maximum number of tokens to consider when sampling.
    ///
    /// Gemini models use Top-p (nucleus) sampling or a combination of Top-k and
    /// nucleus sampling. Top-k sampling considers the set of `top_k` most probable
    /// tokens. Models running with nucleus sampling don't allow top_k setting.
    ///
    /// Note: The default value varies by `Model` and is specified by
    /// the`Model.top_p` attribute returned from the `getModel` function. An empty
    /// `top_k` attribute indicates that the model doesn't apply top-k sampling
    /// and doesn't allow setting `top_k` on requests.
    #[prost(int32, optional, tag = "7")]
    pub top_k: ::core::option::Option<i32>,
    /// Optional. MIME type of the generated candidate text.
    /// Supported MIME types are:
    /// `text/plain`: (default) Text output.
    /// `application/json`: JSON response in the response candidates.
    /// `text/x.enum`: ENUM as a string response in the response candidates.
    /// Refer to the
    /// [docs](<https://ai.google.dev/gemini-api/docs/prompting_with_media#plain_text_formats>)
    /// for a list of all supported text MIME types.
    #[prost(string, tag = "13")]
    pub response_mime_type: ::prost::alloc::string::String,
    /// Optional. Output schema of the generated candidate text. Schemas must be a
    /// subset of the [OpenAPI schema](<https://spec.openapis.org/oas/v3.0.3#schema>)
    /// and can be objects, primitives or arrays.
    ///
    /// If set, a compatible `response_mime_type` must also be set.
    /// Compatible MIME types:
    /// `application/json`: Schema for JSON response.
    /// Refer to the [JSON text generation
    /// guide](<https://ai.google.dev/gemini-api/docs/json-mode>) for more details.
    #[prost(message, optional, tag = "14")]
    pub response_schema: ::core::option::Option<Schema>,
    /// Optional. Presence penalty applied to the next token's logprobs if the
    /// token has already been seen in the response.
    ///
    /// This penalty is binary on/off and not dependant on the number of times the
    /// token is used (after the first). Use
    /// [frequency_penalty][google.ai.generativelanguage.v1beta.GenerationConfig.frequency_penalty]
    /// for a penalty that increases with each use.
    ///
    /// A positive penalty will discourage the use of tokens that have already
    /// been used in the response, increasing the vocabulary.
    ///
    /// A negative penalty will encourage the use of tokens that have already been
    /// used in the response, decreasing the vocabulary.
    #[prost(float, optional, tag = "15")]
    pub presence_penalty: ::core::option::Option<f32>,
    /// Optional. Frequency penalty applied to the next token's logprobs,
    /// multiplied by the number of times each token has been seen in the respponse
    /// so far.
    ///
    /// A positive penalty will discourage the use of tokens that have already
    /// been used, proportional to the number of times the token has been used:
    /// The more a token is used, the more dificult it is for the model to use
    /// that token again increasing the vocabulary of responses.
    ///
    /// Caution: A _negative_ penalty will encourage the model to reuse tokens
    /// proportional to the number of times the token has been used. Small
    /// negative values will reduce the vocabulary of a response. Larger negative
    /// values will cause the model to start repeating a common token  until it
    /// hits the
    /// [max_output_tokens][google.ai.generativelanguage.v1beta.GenerationConfig.max_output_tokens]
    /// limit.
    #[prost(float, optional, tag = "16")]
    pub frequency_penalty: ::core::option::Option<f32>,
    /// Optional. If true, export the logprobs results in response.
    #[prost(bool, optional, tag = "17")]
    pub response_logprobs: ::core::option::Option<bool>,
    /// Optional. Only valid if
    /// [response_logprobs=True][google.ai.generativelanguage.v1beta.GenerationConfig.response_logprobs].
    /// This sets the number of top logprobs to return at each decoding step in the
    /// [Candidate.logprobs_result][google.ai.generativelanguage.v1beta.Candidate.logprobs_result].
    #[prost(int32, optional, tag = "18")]
    pub logprobs: ::core::option::Option<i32>,
    /// Optional. Enables enhanced civic answers. It may not be available for all
    /// models.
    #[prost(bool, optional, tag = "19")]
    pub enable_enhanced_civic_answers: ::core::option::Option<bool>,
    /// Optional. The requested modalities of the response. Represents the set of
    /// modalities that the model can return, and should be expected in the
    /// response. This is an exact match to the modalities of the response.
    ///
    /// A model may have multiple combinations of supported modalities. If the
    /// requested modalities do not match any of the supported combinations, an
    /// error will be returned.
    ///
    /// An empty list is equivalent to requesting only text.
    #[prost(
        enumeration = "generation_config::Modality",
        repeated,
        packed = "false",
        tag = "20"
    )]
    pub response_modalities: ::prost::alloc::vec::Vec<i32>,
    /// Optional. The speech generation config.
    #[prost(message, optional, tag = "21")]
    pub speech_config: ::core::option::Option<SpeechConfig>,
}

/// Configuration for retrieving grounding content from a `Corpus` or
/// `Document` created using the Semantic Retriever API.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SemanticRetrieverConfig {
    /// Required. Name of the resource for retrieval. Example: `corpora/123` or
    /// `corpora/123/documents/abc`.
    #[prost(string, tag = "1")]
    pub source: ::prost::alloc::string::String,
    /// Required. Query to use for matching `Chunk`s in the given resource by
    /// similarity.
    #[prost(message, optional, tag = "2")]
    pub query: ::core::option::Option<Content>,
    /// Optional. Filters for selecting `Document`s and/or `Chunk`s from the
    /// resource.
    #[prost(message, repeated, tag = "3")]
    pub metadata_filters: ::prost::alloc::vec::Vec<MetadataFilter>,
    /// Optional. Maximum number of relevant `Chunk`s to retrieve.
    #[prost(int32, optional, tag = "4")]
    pub max_chunks_count: ::core::option::Option<i32>,
    /// Optional. Minimum relevance score for retrieved relevant `Chunk`s.
    #[prost(float, optional, tag = "5")]
    pub minimum_relevance_score: ::core::option::Option<f32>,
}
/// Response from the model supporting multiple candidate responses.
///
/// Safety ratings and content filtering are reported for both
/// prompt in `GenerateContentResponse.prompt_feedback` and for each candidate
/// in `finish_reason` and in `safety_ratings`. The API:
///   - Returns either all requested candidates or none of them
///   - Returns no candidates at all only if there was something wrong with the
///     prompt (check `prompt_feedback`)
///   - Reports feedback on each candidate in `finish_reason` and
///     `safety_ratings`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateContentResponse {
    /// Candidate responses from the model.
    #[prost(message, repeated, tag = "1")]
    pub candidates: ::prost::alloc::vec::Vec<Candidate>,
    /// Returns the prompt's feedback related to the content filters.
    #[prost(message, optional, tag = "2")]
    pub prompt_feedback: ::core::option::Option<generate_content_response::PromptFeedback>,
    /// Output only. Metadata on the generation requests' token usage.
    #[prost(message, optional, tag = "3")]
    pub usage_metadata: ::core::option::Option<generate_content_response::UsageMetadata>,
    /// Output only. The model version used to generate the response.
    #[prost(string, tag = "4")]
    pub model_version: ::prost::alloc::string::String,
}
/// A response candidate generated from the model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Candidate {
    /// Output only. Index of the candidate in the list of response candidates.
    #[prost(int32, optional, tag = "3")]
    pub index: ::core::option::Option<i32>,
    /// Output only. Generated content returned from the model.
    #[prost(message, optional, tag = "1")]
    pub content: ::core::option::Option<Content>,
    /// Optional. Output only. The reason why the model stopped generating tokens.
    ///
    /// If empty, the model has not stopped generating tokens.
    #[prost(enumeration = "candidate::FinishReason", tag = "2")]
    pub finish_reason: i32,
    /// List of ratings for the safety of a response candidate.
    ///
    /// There is at most one rating per category.
    #[prost(message, repeated, tag = "5")]
    pub safety_ratings: ::prost::alloc::vec::Vec<SafetyRating>,
    /// Output only. Citation information for model-generated candidate.
    ///
    /// This field may be populated with recitation information for any text
    /// included in the `content`. These are passages that are "recited" from
    /// copyrighted material in the foundational LLM's training data.
    #[prost(message, optional, tag = "6")]
    pub citation_metadata: ::core::option::Option<CitationMetadata>,
    /// Output only. Token count for this candidate.
    #[prost(int32, tag = "7")]
    pub token_count: i32,
    /// Output only. Attribution information for sources that contributed to a
    /// grounded answer.
    ///
    /// This field is populated for `GenerateAnswer` calls.
    #[prost(message, repeated, tag = "8")]
    pub grounding_attributions: ::prost::alloc::vec::Vec<GroundingAttribution>,
    /// Output only. Grounding metadata for the candidate.
    ///
    /// This field is populated for `GenerateContent` calls.
    #[prost(message, optional, tag = "9")]
    pub grounding_metadata: ::core::option::Option<GroundingMetadata>,
    /// Output only. Average log probability score of the candidate.
    #[prost(double, tag = "10")]
    pub avg_logprobs: f64,
    /// Output only. Log-likelihood scores for the response tokens and top tokens
    #[prost(message, optional, tag = "11")]
    pub logprobs_result: ::core::option::Option<LogprobsResult>,
}

/// Logprobs Result
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct LogprobsResult {
    /// Length = total number of decoding steps.
    #[prost(message, repeated, tag = "1")]
    pub top_candidates: ::prost::alloc::vec::Vec<logprobs_result::TopCandidates>,
    /// Length = total number of decoding steps.
    /// The chosen candidates may or may not be in top_candidates.
    #[prost(message, repeated, tag = "2")]
    pub chosen_candidates: ::prost::alloc::vec::Vec<logprobs_result::Candidate>,
}

/// Identifier for the source contributing to this attribution.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AttributionSourceId {
    #[prost(oneof = "attribution_source_id::Source", tags = "1, 2")]
    pub source: ::core::option::Option<attribution_source_id::Source>,
}

/// Attribution for a source that contributed to an answer.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundingAttribution {
    /// Output only. Identifier for the source contributing to this attribution.
    #[prost(message, optional, tag = "3")]
    pub source_id: ::core::option::Option<AttributionSourceId>,
    /// Grounding source content that makes up this attribution.
    #[prost(message, optional, tag = "2")]
    pub content: ::core::option::Option<Content>,
}
/// Metadata related to retrieval in the grounding flow.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct RetrievalMetadata {
    /// Optional. Score indicating how likely information from google search could
    /// help answer the prompt. The score is in the range \[0, 1\], where 0 is the
    /// least likely and 1 is the most likely. This score is only populated when
    /// google search grounding and dynamic retrieval is enabled. It will be
    /// compared to the threshold to determine whether to trigger google search.
    #[prost(float, tag = "2")]
    pub google_search_dynamic_retrieval_score: f32,
}
/// Metadata returned to client when grounding is enabled.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundingMetadata {
    /// Optional. Google search entry for the following-up web searches.
    #[prost(message, optional, tag = "1")]
    pub search_entry_point: ::core::option::Option<SearchEntryPoint>,
    /// List of supporting references retrieved from specified grounding source.
    #[prost(message, repeated, tag = "2")]
    pub grounding_chunks: ::prost::alloc::vec::Vec<GroundingChunk>,
    /// List of grounding support.
    #[prost(message, repeated, tag = "3")]
    pub grounding_supports: ::prost::alloc::vec::Vec<GroundingSupport>,
    /// Metadata related to retrieval in the grounding flow.
    #[prost(message, optional, tag = "4")]
    pub retrieval_metadata: ::core::option::Option<RetrievalMetadata>,
    /// Web search queries for the following-up web search.
    #[prost(string, repeated, tag = "5")]
    pub web_search_queries: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Google search entry point.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SearchEntryPoint {
    /// Optional. Web content snippet that can be embedded in a web page or an app
    /// webview.
    #[prost(string, tag = "1")]
    pub rendered_content: ::prost::alloc::string::String,
    /// Optional. Base64 encoded JSON representing array of <search term, search
    /// url> tuple.
    #[prost(bytes = "vec", tag = "2")]
    pub sdk_blob: ::prost::alloc::vec::Vec<u8>,
}
/// Grounding chunk.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundingChunk {
    /// Chunk type.
    #[prost(oneof = "grounding_chunk::ChunkType", tags = "1")]
    pub chunk_type: ::core::option::Option<grounding_chunk::ChunkType>,
}

/// Segment of the content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Segment {
    /// Output only. The index of a Part object within its parent Content object.
    #[prost(int32, tag = "1")]
    pub part_index: i32,
    /// Output only. Start index in the given Part, measured in bytes. Offset from
    /// the start of the Part, inclusive, starting at zero.
    #[prost(int32, tag = "2")]
    pub start_index: i32,
    /// Output only. End index in the given Part, measured in bytes. Offset from
    /// the start of the Part, exclusive, starting at zero.
    #[prost(int32, tag = "3")]
    pub end_index: i32,
    /// Output only. The text corresponding to the segment from the response.
    #[prost(string, tag = "4")]
    pub text: ::prost::alloc::string::String,
}
/// Grounding support.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundingSupport {
    /// Segment of the content this support belongs to.
    #[prost(message, optional, tag = "1")]
    pub segment: ::core::option::Option<Segment>,
    /// A list of indices (into 'grounding_chunk') specifying the
    /// citations associated with the claim. For instance \[1,3,4\] means
    /// that grounding_chunk\[1\], grounding_chunk\[3\],
    /// grounding_chunk\[4\] are the retrieved content attributed to the claim.
    #[prost(int32, repeated, tag = "2")]
    pub grounding_chunk_indices: ::prost::alloc::vec::Vec<i32>,
    /// Confidence score of the support references. Ranges from 0 to 1. 1 is the
    /// most confident. This list must have the same size as the
    /// grounding_chunk_indices.
    #[prost(float, repeated, tag = "3")]
    pub confidence_scores: ::prost::alloc::vec::Vec<f32>,
}
/// Request to generate a grounded answer from the `Model`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateAnswerRequest {
    /// Required. The name of the `Model` to use for generating the grounded
    /// response.
    ///
    /// Format: `model=models/{model}`.
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Required. The content of the current conversation with the `Model`. For
    /// single-turn queries, this is a single question to answer. For multi-turn
    /// queries, this is a repeated field that contains conversation history and
    /// the last `Content` in the list containing the question.
    ///
    /// Note: `GenerateAnswer` only supports queries in English.
    #[prost(message, repeated, tag = "2")]
    pub contents: ::prost::alloc::vec::Vec<Content>,
    /// Required. Style in which answers should be returned.
    #[prost(enumeration = "generate_answer_request::AnswerStyle", tag = "5")]
    pub answer_style: i32,
    /// Optional. A list of unique `SafetySetting` instances for blocking unsafe
    /// content.
    ///
    /// This will be enforced on the `GenerateAnswerRequest.contents` and
    /// `GenerateAnswerResponse.candidate`. There should not be more than one
    /// setting for each `SafetyCategory` type. The API will block any contents and
    /// responses that fail to meet the thresholds set by these settings. This list
    /// overrides the default settings for each `SafetyCategory` specified in the
    /// safety_settings. If there is no `SafetySetting` for a given
    /// `SafetyCategory` provided in the list, the API will use the default safety
    /// setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
    /// HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
    /// HARM_CATEGORY_HARASSMENT are supported.
    /// Refer to the
    /// [guide](<https://ai.google.dev/gemini-api/docs/safety-settings>)
    /// for detailed information on available safety settings. Also refer to the
    /// [Safety guidance](<https://ai.google.dev/gemini-api/docs/safety-guidance>) to
    /// learn how to incorporate safety considerations in your AI applications.
    #[prost(message, repeated, tag = "3")]
    pub safety_settings: ::prost::alloc::vec::Vec<SafetySetting>,
    /// Optional. Controls the randomness of the output.
    ///
    /// Values can range from \[0.0,1.0\], inclusive. A value closer to 1.0 will
    /// produce responses that are more varied and creative, while a value closer
    /// to 0.0 will typically result in more straightforward responses from the
    /// model. A low temperature (~0.2) is usually recommended for
    /// Attributed-Question-Answering use cases.
    #[prost(float, optional, tag = "4")]
    pub temperature: ::core::option::Option<f32>,
    /// The sources in which to ground the answer.
    #[prost(oneof = "generate_answer_request::GroundingSource", tags = "6, 7")]
    pub grounding_source: ::core::option::Option<generate_answer_request::GroundingSource>,
}

/// Response from the model for a grounded answer.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateAnswerResponse {
    /// Candidate answer from the model.
    ///
    /// Note: The model *always* attempts to provide a grounded answer, even when
    /// the answer is unlikely to be answerable from the given passages.
    /// In that case, a low-quality or ungrounded answer may be provided, along
    /// with a low `answerable_probability`.
    #[prost(message, optional, tag = "1")]
    pub answer: ::core::option::Option<Candidate>,
    /// Output only. The model's estimate of the probability that its answer is
    /// correct and grounded in the input passages.
    ///
    /// A low `answerable_probability` indicates that the answer might not be
    /// grounded in the sources.
    ///
    /// When `answerable_probability` is low, you may want to:
    ///
    /// * Display a message to the effect of "We couldn’t answer that question" to
    /// the user.
    /// * Fall back to a general-purpose LLM that answers the question from world
    /// knowledge. The threshold and nature of such fallbacks will depend on
    /// individual use cases. `0.5` is a good starting threshold.
    #[prost(float, optional, tag = "2")]
    pub answerable_probability: ::core::option::Option<f32>,
    /// Output only. Feedback related to the input data used to answer the
    /// question, as opposed to the model-generated response to the question.
    ///
    /// The input data can be one or more of the following:
    ///
    /// - Question specified by the last entry in `GenerateAnswerRequest.content`
    /// - Conversation history specified by the other entries in
    /// `GenerateAnswerRequest.content`
    /// - Grounding sources (`GenerateAnswerRequest.semantic_retriever` or
    /// `GenerateAnswerRequest.inline_passages`)
    #[prost(message, optional, tag = "3")]
    pub input_feedback: ::core::option::Option<generate_answer_response::InputFeedback>,
}

/// Request containing the `Content` for the model to embed.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EmbedContentRequest {
    /// Required. The model's resource name. This serves as an ID for the Model to
    /// use.
    ///
    /// This name should match a model name returned by the `ListModels` method.
    ///
    /// Format: `models/{model}`
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Required. The content to embed. Only the `parts.text` fields will be
    /// counted.
    #[prost(message, optional, tag = "2")]
    pub content: ::core::option::Option<Content>,
    /// Optional. Optional task type for which the embeddings will be used. Can
    /// only be set for `models/embedding-001`.
    #[prost(enumeration = "TaskType", optional, tag = "3")]
    pub task_type: ::core::option::Option<i32>,
    /// Optional. An optional title for the text. Only applicable when TaskType is
    /// `RETRIEVAL_DOCUMENT`.
    ///
    /// Note: Specifying a `title` for `RETRIEVAL_DOCUMENT` provides better quality
    /// embeddings for retrieval.
    #[prost(string, optional, tag = "4")]
    pub title: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Optional reduced dimension for the output embedding. If set,
    /// excessive values in the output embedding are truncated from the end.
    /// Supported by newer models since 2024 only. You cannot set this value if
    /// using the earlier model (`models/embedding-001`).
    #[prost(int32, optional, tag = "5")]
    pub output_dimensionality: ::core::option::Option<i32>,
}
/// A list of floats representing an embedding.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ContentEmbedding {
    /// The embedding values.
    #[prost(float, repeated, tag = "1")]
    pub values: ::prost::alloc::vec::Vec<f32>,
}
/// The response to an `EmbedContentRequest`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EmbedContentResponse {
    /// Output only. The embedding generated from the input content.
    #[prost(message, optional, tag = "1")]
    pub embedding: ::core::option::Option<ContentEmbedding>,
}
/// Batch request to get embeddings from the model for a list of prompts.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchEmbedContentsRequest {
    /// Required. The model's resource name. This serves as an ID for the Model to
    /// use.
    ///
    /// This name should match a model name returned by the `ListModels` method.
    ///
    /// Format: `models/{model}`
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Required. Embed requests for the batch. The model in each of these requests
    /// must match the model specified `BatchEmbedContentsRequest.model`.
    #[prost(message, repeated, tag = "2")]
    pub requests: ::prost::alloc::vec::Vec<EmbedContentRequest>,
}
/// The response to a `BatchEmbedContentsRequest`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchEmbedContentsResponse {
    /// Output only. The embeddings for each request, in the same order as provided
    /// in the batch request.
    #[prost(message, repeated, tag = "1")]
    pub embeddings: ::prost::alloc::vec::Vec<ContentEmbedding>,
}
/// Counts the number of tokens in the `prompt` sent to a model.
///
/// Models may tokenize text differently, so each model may return a different
/// `token_count`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CountTokensRequest {
    /// Required. The model's resource name. This serves as an ID for the Model to
    /// use.
    ///
    /// This name should match a model name returned by the `ListModels` method.
    ///
    /// Format: `models/{model}`
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Optional. The input given to the model as a prompt. This field is ignored
    /// when `generate_content_request` is set.
    #[prost(message, repeated, tag = "2")]
    pub contents: ::prost::alloc::vec::Vec<Content>,
    /// Optional. The overall input given to the `Model`. This includes the prompt
    /// as well as other model steering information like [system
    /// instructions](<https://ai.google.dev/gemini-api/docs/system-instructions>),
    /// and/or function declarations for [function
    /// calling](<https://ai.google.dev/gemini-api/docs/function-calling>).
    /// `Model`s/`Content`s and `generate_content_request`s are mutually
    /// exclusive. You can either send `Model` + `Content`s or a
    /// `generate_content_request`, but never both.
    #[prost(message, optional, tag = "3")]
    pub generate_content_request: ::core::option::Option<GenerateContentRequest>,
}
/// A response from `CountTokens`.
///
/// It returns the model's `token_count` for the `prompt`.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct CountTokensResponse {
    /// The number of tokens that the `Model` tokenizes the `prompt` into. Always
    /// non-negative.
    #[prost(int32, tag = "1")]
    pub total_tokens: i32,
    /// Number of tokens in the cached part of the prompt (the cached content).
    #[prost(int32, tag = "5")]
    pub cached_content_token_count: i32,
}
/// Type of task for which the embedding will be used.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum TaskType {
    /// Unset value, which will default to one of the other enum values.
    Unspecified = 0,
    /// Specifies the given text is a query in a search/retrieval setting.
    RetrievalQuery = 1,
    /// Specifies the given text is a document from the corpus being searched.
    RetrievalDocument = 2,
    /// Specifies the given text will be used for STS.
    SemanticSimilarity = 3,
    /// Specifies that the given text will be classified.
    Classification = 4,
    /// Specifies that the embeddings will be used for clustering.
    Clustering = 5,
    /// Specifies that the given text will be used for question answering.
    QuestionAnswering = 6,
    /// Specifies that the given text will be used for fact verification.
    FactVerification = 7,
}
impl TaskType {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "TASK_TYPE_UNSPECIFIED",
            Self::RetrievalQuery => "RETRIEVAL_QUERY",
            Self::RetrievalDocument => "RETRIEVAL_DOCUMENT",
            Self::SemanticSimilarity => "SEMANTIC_SIMILARITY",
            Self::Classification => "CLASSIFICATION",
            Self::Clustering => "CLUSTERING",
            Self::QuestionAnswering => "QUESTION_ANSWERING",
            Self::FactVerification => "FACT_VERIFICATION",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "TASK_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
            "RETRIEVAL_QUERY" => Some(Self::RetrievalQuery),
            "RETRIEVAL_DOCUMENT" => Some(Self::RetrievalDocument),
            "SEMANTIC_SIMILARITY" => Some(Self::SemanticSimilarity),
            "CLASSIFICATION" => Some(Self::Classification),
            "CLUSTERING" => Some(Self::Clustering),
            "QUESTION_ANSWERING" => Some(Self::QuestionAnswering),
            "FACT_VERIFICATION" => Some(Self::FactVerification),
            _ => None,
        }
    }
}

/// Content that has been preprocessed and can be used in subsequent request
/// to GenerativeService.
///
/// Cached content can be only used with model it was created for.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CachedContent {
    /// Optional. Identifier. The resource name referring to the cached content.
    /// Format: `cachedContents/{id}`
    #[prost(string, optional, tag = "1")]
    pub name: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Immutable. The user-generated meaningful display name of the
    /// cached content. Maximum 128 Unicode characters.
    #[prost(string, optional, tag = "11")]
    pub display_name: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Immutable. The name of the `Model` to use for cached content
    /// Format: `models/{model}`
    #[prost(string, optional, tag = "2")]
    pub model: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Input only. Immutable. Developer set system instruction.
    /// Currently text only.
    #[prost(message, optional, tag = "3")]
    pub system_instruction: ::core::option::Option<Content>,
    /// Optional. Input only. Immutable. The content to cache.
    #[prost(message, repeated, tag = "4")]
    pub contents: ::prost::alloc::vec::Vec<Content>,
    /// Optional. Input only. Immutable. A list of `Tools` the model may use to
    /// generate the next response
    #[prost(message, repeated, tag = "5")]
    pub tools: ::prost::alloc::vec::Vec<Tool>,
    /// Optional. Input only. Immutable. Tool config. This config is shared for all
    /// tools.
    #[prost(message, optional, tag = "6")]
    pub tool_config: ::core::option::Option<ToolConfig>,
    /// Output only. Creation time of the cache entry.
    #[prost(message, optional, tag = "7")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. When the cache entry was last updated in UTC time.
    #[prost(message, optional, tag = "8")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Metadata on the usage of the cached content.
    #[prost(message, optional, tag = "12")]
    pub usage_metadata: ::core::option::Option<cached_content::UsageMetadata>,
    /// Specifies when this resource will expire.
    #[prost(oneof = "cached_content::Expiration", tags = "9, 10")]
    pub expiration: ::core::option::Option<cached_content::Expiration>,
}

/// Request to list CachedContents.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListCachedContentsRequest {
    /// Optional. The maximum number of cached contents to return. The service may
    /// return fewer than this value. If unspecified, some default (under maximum)
    /// number of items will be returned. The maximum value is 1000; values above
    /// 1000 will be coerced to 1000.
    #[prost(int32, tag = "1")]
    pub page_size: i32,
    /// Optional. A page token, received from a previous `ListCachedContents` call.
    /// Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to `ListCachedContents` must
    /// match the call that provided the page token.
    #[prost(string, tag = "2")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response with CachedContents list.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListCachedContentsResponse {
    /// List of cached contents.
    #[prost(message, repeated, tag = "1")]
    pub cached_contents: ::prost::alloc::vec::Vec<CachedContent>,
    /// A token, which can be sent as `page_token` to retrieve the next page.
    /// If this field is omitted, there are no subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request to create CachedContent.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateCachedContentRequest {
    /// Required. The cached content to create.
    #[prost(message, optional, tag = "1")]
    pub cached_content: ::core::option::Option<CachedContent>,
}
/// Request to read CachedContent.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetCachedContentRequest {
    /// Required. The resource name referring to the content cache entry.
    /// Format: `cachedContents/{id}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request to update CachedContent.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateCachedContentRequest {
    /// Required. The content cache entry to update
    #[prost(message, optional, tag = "1")]
    pub cached_content: ::core::option::Option<CachedContent>,
    /// The list of fields to update.
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request to delete CachedContent.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteCachedContentRequest {
    /// Required. The resource name referring to the content cache entry
    /// Format: `cachedContents/{id}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}

/// A file uploaded to the API.
/// Next ID: 15
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct File {
    /// Immutable. Identifier. The `File` resource name. The ID (name excluding the
    /// "files/" prefix) can contain up to 40 characters that are lowercase
    /// alphanumeric or dashes (-). The ID cannot start or end with a dash. If the
    /// name is empty on create, a unique name will be generated. Example:
    /// `files/123-456`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. The human-readable display name for the `File`. The display name
    /// must be no more than 512 characters in length, including spaces. Example:
    /// "Welcome Image"
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Output only. MIME type of the file.
    #[prost(string, tag = "3")]
    pub mime_type: ::prost::alloc::string::String,
    /// Output only. Size of the file in bytes.
    #[prost(int64, tag = "4")]
    pub size_bytes: i64,
    /// Output only. The timestamp of when the `File` was created.
    #[prost(message, optional, tag = "5")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The timestamp of when the `File` was last updated.
    #[prost(message, optional, tag = "6")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The timestamp of when the `File` will be deleted. Only set if
    /// the `File` is scheduled to expire.
    #[prost(message, optional, tag = "7")]
    pub expiration_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. SHA-256 hash of the uploaded bytes.
    #[prost(bytes = "vec", tag = "8")]
    pub sha256_hash: ::prost::alloc::vec::Vec<u8>,
    /// Output only. The uri of the `File`.
    #[prost(string, tag = "9")]
    pub uri: ::prost::alloc::string::String,
    /// Output only. Processing state of the File.
    #[prost(enumeration = "file::State", tag = "10")]
    pub state: i32,
    /// Output only. Error status if File processing failed.
    #[prost(message, optional, tag = "11")]
    pub error: ::core::option::Option<rpc::Status>,
    /// Metadata for the File.
    #[prost(oneof = "file::Metadata", tags = "12")]
    pub metadata: ::core::option::Option<file::Metadata>,
}

/// Metadata for a video `File`.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct VideoMetadata {
    /// Duration of the video.
    #[prost(message, optional, tag = "1")]
    pub video_duration: ::core::option::Option<::prost_types::Duration>,
}
/// Request for `CreateFile`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateFileRequest {
    /// Optional. Metadata for the file to create.
    #[prost(message, optional, tag = "1")]
    pub file: ::core::option::Option<File>,
}
/// Response for `CreateFile`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateFileResponse {
    /// Metadata for the created file.
    #[prost(message, optional, tag = "1")]
    pub file: ::core::option::Option<File>,
}
/// Request for `ListFiles`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFilesRequest {
    /// Optional. Maximum number of `File`s to return per page.
    /// If unspecified, defaults to 10. Maximum `page_size` is 100.
    #[prost(int32, tag = "1")]
    pub page_size: i32,
    /// Optional. A page token from a previous `ListFiles` call.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response for `ListFiles`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFilesResponse {
    /// The list of `File`s.
    #[prost(message, repeated, tag = "1")]
    pub files: ::prost::alloc::vec::Vec<File>,
    /// A token that can be sent as a `page_token` into a subsequent `ListFiles`
    /// call.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request for `GetFile`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetFileRequest {
    /// Required. The name of the `File` to get.
    /// Example: `files/abc-123`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request for `DeleteFile`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteFileRequest {
    /// Required. The name of the `File` to delete.
    /// Example: `files/abc-123`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
// This file is @generated by prost-build.
/// Information about a Generative Language Model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Model {
    /// Required. The resource name of the `Model`. Refer to [Model
    /// variants](<https://ai.google.dev/gemini-api/docs/models/gemini#model-variations>)
    /// for all allowed values.
    ///
    /// Format: `models/{model}` with a `{model}` naming convention of:
    ///
    /// * "{base_model_id}-{version}"
    ///
    /// Examples:
    ///
    /// * `models/gemini-1.5-flash-001`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The name of the base model, pass this to the generation request.
    ///
    /// Examples:
    ///
    /// * `gemini-1.5-flash`
    #[prost(string, tag = "2")]
    pub base_model_id: ::prost::alloc::string::String,
    /// Required. The version number of the model.
    ///
    /// This represents the major version (`1.0` or `1.5`)
    #[prost(string, tag = "3")]
    pub version: ::prost::alloc::string::String,
    /// The human-readable name of the model. E.g. "Gemini 1.5 Flash".
    ///
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "4")]
    pub display_name: ::prost::alloc::string::String,
    /// A short description of the model.
    #[prost(string, tag = "5")]
    pub description: ::prost::alloc::string::String,
    /// Maximum number of input tokens allowed for this model.
    #[prost(int32, tag = "6")]
    pub input_token_limit: i32,
    /// Maximum number of output tokens available for this model.
    #[prost(int32, tag = "7")]
    pub output_token_limit: i32,
    /// The model's supported generation methods.
    ///
    /// The corresponding API method names are defined as Pascal case
    /// strings, such as `generateMessage` and `generateContent`.
    #[prost(string, repeated, tag = "8")]
    pub supported_generation_methods: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Controls the randomness of the output.
    ///
    /// Values can range over `\[0.0,max_temperature\]`, inclusive. A higher value
    /// will produce responses that are more varied, while a value closer to `0.0`
    /// will typically result in less surprising responses from the model.
    /// This value specifies default to be used by the backend while making the
    /// call to the model.
    #[prost(float, optional, tag = "9")]
    pub temperature: ::core::option::Option<f32>,
    /// The maximum temperature this model can use.
    #[prost(float, optional, tag = "13")]
    pub max_temperature: ::core::option::Option<f32>,
    /// For [Nucleus
    /// sampling](<https://ai.google.dev/gemini-api/docs/prompting-strategies#top-p>).
    ///
    /// Nucleus sampling considers the smallest set of tokens whose probability
    /// sum is at least `top_p`.
    /// This value specifies default to be used by the backend while making the
    /// call to the model.
    #[prost(float, optional, tag = "10")]
    pub top_p: ::core::option::Option<f32>,
    /// For Top-k sampling.
    ///
    /// Top-k sampling considers the set of `top_k` most probable tokens.
    /// This value specifies default to be used by the backend while making the
    /// call to the model.
    /// If empty, indicates the model doesn't use top-k sampling, and `top_k` isn't
    /// allowed as a generation parameter.
    #[prost(int32, optional, tag = "11")]
    pub top_k: ::core::option::Option<i32>,
}
/// A fine-tuned model created using ModelService.CreateTunedModel.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TunedModel {
    /// Output only. The tuned model name. A unique name will be generated on
    /// create. Example: `tunedModels/az2mb0bpw6i` If display_name is set on
    /// create, the id portion of the name will be set by concatenating the words
    /// of the display_name with hyphens and adding a random portion for
    /// uniqueness.
    ///
    /// Example:
    ///
    ///   * display_name = `Sentence Translator`
    ///   * name = `tunedModels/sentence-translator-u3b7m`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. The name to display for this model in user interfaces.
    /// The display name must be up to 40 characters including spaces.
    #[prost(string, tag = "5")]
    pub display_name: ::prost::alloc::string::String,
    /// Optional. A short description of this model.
    #[prost(string, tag = "6")]
    pub description: ::prost::alloc::string::String,
    /// Optional. Controls the randomness of the output.
    ///
    /// Values can range over `\[0.0,1.0\]`, inclusive. A value closer to `1.0` will
    /// produce responses that are more varied, while a value closer to `0.0` will
    /// typically result in less surprising responses from the model.
    ///
    /// This value specifies default to be the one used by the base model while
    /// creating the model.
    #[prost(float, optional, tag = "11")]
    pub temperature: ::core::option::Option<f32>,
    /// Optional. For Nucleus sampling.
    ///
    /// Nucleus sampling considers the smallest set of tokens whose probability
    /// sum is at least `top_p`.
    ///
    /// This value specifies default to be the one used by the base model while
    /// creating the model.
    #[prost(float, optional, tag = "12")]
    pub top_p: ::core::option::Option<f32>,
    /// Optional. For Top-k sampling.
    ///
    /// Top-k sampling considers the set of `top_k` most probable tokens.
    /// This value specifies default to be used by the backend while making the
    /// call to the model.
    ///
    /// This value specifies default to be the one used by the base model while
    /// creating the model.
    #[prost(int32, optional, tag = "13")]
    pub top_k: ::core::option::Option<i32>,
    /// Output only. The state of the tuned model.
    #[prost(enumeration = "tuned_model::State", tag = "7")]
    pub state: i32,
    /// Output only. The timestamp when this model was created.
    #[prost(message, optional, tag = "8")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The timestamp when this model was updated.
    #[prost(message, optional, tag = "9")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Required. The tuning task that creates the tuned model.
    #[prost(message, optional, tag = "10")]
    pub tuning_task: ::core::option::Option<TuningTask>,
    /// Optional. List of project numbers that have read access to the tuned model.
    #[prost(int64, repeated, packed = "false", tag = "14")]
    pub reader_project_numbers: ::prost::alloc::vec::Vec<i64>,
    /// The model used as the starting point for tuning.
    #[prost(oneof = "tuned_model::SourceModel", tags = "3, 4")]
    pub source_model: ::core::option::Option<tuned_model::SourceModel>,
}
/// Tuned model as a source for training a new model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TunedModelSource {
    /// Immutable. The name of the `TunedModel` to use as the starting point for
    /// training the new model.
    /// Example: `tunedModels/my-tuned-model`
    #[prost(string, tag = "1")]
    pub tuned_model: ::prost::alloc::string::String,
    /// Output only. The name of the base `Model` this `TunedModel` was tuned from.
    /// Example: `models/gemini-1.5-flash-001`
    #[prost(string, tag = "2")]
    pub base_model: ::prost::alloc::string::String,
}
/// Tuning tasks that create tuned models.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TuningTask {
    /// Output only. The timestamp when tuning this model started.
    #[prost(message, optional, tag = "1")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The timestamp when tuning this model completed.
    #[prost(message, optional, tag = "2")]
    pub complete_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Metrics collected during tuning.
    #[prost(message, repeated, tag = "3")]
    pub snapshots: ::prost::alloc::vec::Vec<TuningSnapshot>,
    /// Required. Input only. Immutable. The model training data.
    #[prost(message, optional, tag = "4")]
    pub training_data: ::core::option::Option<Dataset>,
    /// Immutable. Hyperparameters controlling the tuning process. If not provided,
    /// default values will be used.
    #[prost(message, optional, tag = "5")]
    pub hyperparameters: ::core::option::Option<Hyperparameters>,
}
/// Hyperparameters controlling the tuning process. Read more at
/// <https://ai.google.dev/docs/model_tuning_guidance>
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct Hyperparameters {
    /// Immutable. The number of training epochs. An epoch is one pass through the
    /// training data. If not set, a default of 5 will be used.
    #[prost(int32, optional, tag = "14")]
    pub epoch_count: ::core::option::Option<i32>,
    /// Immutable. The batch size hyperparameter for tuning.
    /// If not set, a default of 4 or 16 will be used based on the number of
    /// training examples.
    #[prost(int32, optional, tag = "15")]
    pub batch_size: ::core::option::Option<i32>,
    /// Options for specifying learning rate during tuning.
    #[prost(oneof = "hyperparameters::LearningRateOption", tags = "16, 17")]
    pub learning_rate_option: ::core::option::Option<hyperparameters::LearningRateOption>,
}
/// Dataset for training or validation.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Dataset {
    /// Inline data or a reference to the data.
    #[prost(oneof = "dataset::Dataset", tags = "1")]
    pub dataset: ::core::option::Option<dataset::Dataset>,
}
/// A set of tuning examples. Can be training or validation data.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TuningExamples {
    /// The examples. Example input can be for text or discuss, but all examples
    /// in a set must be of the same type.
    #[prost(message, repeated, tag = "1")]
    pub examples: ::prost::alloc::vec::Vec<TuningExample>,
}
/// A single example for tuning.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TuningExample {
    /// Required. The expected model output.
    #[prost(string, tag = "3")]
    pub output: ::prost::alloc::string::String,
    /// The input to the model for this example.
    #[prost(oneof = "tuning_example::ModelInput", tags = "1")]
    pub model_input: ::core::option::Option<tuning_example::ModelInput>,
}
/// Record for a single tuning step.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct TuningSnapshot {
    /// Output only. The tuning step.
    #[prost(int32, tag = "1")]
    pub step: i32,
    /// Output only. The epoch this step was part of.
    #[prost(int32, tag = "2")]
    pub epoch: i32,
    /// Output only. The mean loss of the training examples for this step.
    #[prost(float, tag = "3")]
    pub mean_loss: f32,
    /// Output only. The timestamp when this metric was computed.
    #[prost(message, optional, tag = "4")]
    pub compute_time: ::core::option::Option<::prost_types::Timestamp>,
}
/// Request for getting information about a specific Model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetModelRequest {
    /// Required. The resource name of the model.
    ///
    /// This name should match a model name returned by the `ListModels` method.
    ///
    /// Format: `models/{model}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request for listing all Models.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelsRequest {
    /// The maximum number of `Models` to return (per page).
    ///
    /// If unspecified, 50 models will be returned per page.
    /// This method returns at most 1000 models per page, even if you pass a larger
    /// page_size.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// A page token, received from a previous `ListModels` call.
    ///
    /// Provide the `page_token` returned by one request as an argument to the next
    /// request to retrieve the next page.
    ///
    /// When paginating, all other parameters provided to `ListModels` must match
    /// the call that provided the page token.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response from `ListModel` containing a paginated list of Models.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelsResponse {
    /// The returned Models.
    #[prost(message, repeated, tag = "1")]
    pub models: ::prost::alloc::vec::Vec<Model>,
    /// A token, which can be sent as `page_token` to retrieve the next page.
    ///
    /// If this field is omitted, there are no more pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request for getting information about a specific Model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetTunedModelRequest {
    /// Required. The resource name of the model.
    ///
    /// Format: `tunedModels/my-model-id`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request for listing TunedModels.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTunedModelsRequest {
    /// Optional. The maximum number of `TunedModels` to return (per page).
    /// The service may return fewer tuned models.
    ///
    /// If unspecified, at most 10 tuned models will be returned.
    /// This method returns at most 1000 models per page, even if you pass a larger
    /// page_size.
    #[prost(int32, tag = "1")]
    pub page_size: i32,
    /// Optional. A page token, received from a previous `ListTunedModels` call.
    ///
    /// Provide the `page_token` returned by one request as an argument to the next
    /// request to retrieve the next page.
    ///
    /// When paginating, all other parameters provided to `ListTunedModels`
    /// must match the call that provided the page token.
    #[prost(string, tag = "2")]
    pub page_token: ::prost::alloc::string::String,
    /// Optional. A filter is a full text search over the tuned model's description
    /// and display name. By default, results will not include tuned models shared
    /// with everyone.
    ///
    /// Additional operators:
    ///    - owner:me
    ///    - writers:me
    ///    - readers:me
    ///    - readers:everyone
    ///
    /// Examples:
    ///    "owner:me" returns all tuned models to which caller has owner role
    ///    "readers:me" returns all tuned models to which caller has reader role
    ///    "readers:everyone" returns all tuned models that are shared with everyone
    #[prost(string, tag = "3")]
    pub filter: ::prost::alloc::string::String,
}
/// Response from `ListTunedModels` containing a paginated list of Models.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTunedModelsResponse {
    /// The returned Models.
    #[prost(message, repeated, tag = "1")]
    pub tuned_models: ::prost::alloc::vec::Vec<TunedModel>,
    /// A token, which can be sent as `page_token` to retrieve the next page.
    ///
    /// If this field is omitted, there are no more pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request to create a TunedModel.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateTunedModelRequest {
    /// Optional. The unique id for the tuned model if specified.
    /// This value should be up to 40 characters, the first character must be a
    /// letter, the last could be a letter or a number. The id must match the
    /// regular expression: `[a-z](\[a-z0-9-\]{0,38}\[a-z0-9\])?`.
    #[prost(string, optional, tag = "1")]
    pub tuned_model_id: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. The tuned model to create.
    #[prost(message, optional, tag = "2")]
    pub tuned_model: ::core::option::Option<TunedModel>,
}
/// Metadata about the state and progress of creating a tuned model returned from
/// the long-running operation
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateTunedModelMetadata {
    /// Name of the tuned model associated with the tuning operation.
    #[prost(string, tag = "5")]
    pub tuned_model: ::prost::alloc::string::String,
    /// The total number of tuning steps.
    #[prost(int32, tag = "1")]
    pub total_steps: i32,
    /// The number of steps completed.
    #[prost(int32, tag = "2")]
    pub completed_steps: i32,
    /// The completed percentage for the tuning operation.
    #[prost(float, tag = "3")]
    pub completed_percent: f32,
    /// Metrics collected during tuning.
    #[prost(message, repeated, tag = "4")]
    pub snapshots: ::prost::alloc::vec::Vec<TuningSnapshot>,
}
/// Request to update a TunedModel.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateTunedModelRequest {
    /// Required. The tuned model to update.
    #[prost(message, optional, tag = "1")]
    pub tuned_model: ::core::option::Option<TunedModel>,
    /// Optional. The list of fields to update.
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request to delete a TunedModel.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteTunedModelRequest {
    /// Required. The resource name of the model.
    /// Format: `tunedModels/my-model-id`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
